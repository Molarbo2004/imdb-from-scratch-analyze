{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "553d7549",
   "metadata": {},
   "source": [
    "## Математическая основа логистической регрессии\n",
    "\n",
    "Логистическая регрессия — это **линейный классификатор**, который моделирует **вероятность принадлежности объекта к положительному классу** с помощью **логистической (сигмоидной) функции**.\n",
    "\n",
    "### Основная идея\n",
    "\n",
    "В отличие от линейной регрессии, логистическая регрессия **не предсказывает значение напрямую**, а оценивает **вероятность** P(y = 1 | x), где:  \n",
    "- **x** = (x₁, x₂, ..., xₙ) — признаковый вектор (например, TF-IDF веса слов),  \n",
    "- y ∈ {0, 1} — метка класса (0 — негативный, 1 — позитивный отзыв).\n",
    "\n",
    "Модель вычисляет **линейную комбинацию признаков**:\n",
    "\n",
    "$$\n",
    "z = w_1 x_1 + w_2 x_2 + \\ldots + w_n x_n + b\n",
    "$$\n",
    "\n",
    "Затем применяет **сигмоидную функцию** σ(z) = 1 / (1 + exp(-z)), чтобы преобразовать z в вероятность:\n",
    "\n",
    "$$\n",
    "P(y = 1 \\mid \\mathbf{x}) = \\frac{1}{1 + e^{-(w_1 x_1 + \\ldots + w_n x_n + b)}}\n",
    "$$\n",
    "\n",
    "Классификация происходит по порогу 0.5:  \n",
    "- если вероятность ≥ 0.5 → **1 (позитивный)**,  \n",
    "- иначе → **0 (негативный)**.\n",
    "\n",
    "### Обучение: минимизация функции потерь\n",
    "\n",
    "Модель обучается путём минимизации **логистических потерь** (binary cross-entropy):\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{p}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{p}^{(i)}) \\right]\n",
    "$$\n",
    "\n",
    "где:  \n",
    "- m — число примеров,  \n",
    "- y⁽ⁱ⁾ — истинная метка,  \n",
    "- p̂⁽ⁱ⁾ — предсказанная вероятность.\n",
    "\n",
    "Функция потерь **выпуклая**, поэтому оптимизация (градиентный спуск) находит **глобальный минимум**.\n",
    "\n",
    "### Регуляризация\n",
    "\n",
    "Для текстовых данных (тысячи признаков) часто используется **L2-регуляризация**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{reg}} = \\mathcal{L} + \\frac{\\lambda}{2} (w_1^2 + w_2^2 + \\ldots + w_n^2)\n",
    "$$\n",
    "\n",
    "где λ > 0 — параметр регуляризации (чем больше λ, тем сильнее штраф за большие веса).\n",
    "\n",
    "### Почему подходит для анализа тональности?\n",
    "\n",
    "- Отлично работает с **разреженными векторами** (TF-IDF),  \n",
    "- Даёт **надёжные вероятности** (важно для ROC/AUC),  \n",
    "- **Интерпретируема**: положительные веса → слова, связанные с позитивом, отрицательные → с негативом,  \n",
    "- **Быстро обучается** даже на больших данных.\n",
    "\n",
    "> **Интуиция**: каждое слово \"голосует\" за или против позитивной тональности, и вес показывает силу и направление этого голоса."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a7ce49",
   "metadata": {},
   "source": [
    "## Простой пример: как логистическая регрессия классифицирует отзыв?\n",
    "\n",
    "Представим, что модель уже обучена и «знает», насколько каждое слово влияет на тональность.  \n",
    "У неё есть **веса** (коэффициенты) для ключевых слов:\n",
    "\n",
    "| Слово        | Вес (w) |\n",
    "|--------------|--------|\n",
    "| **amazing**  | +2.1   |\n",
    "| **perfect**  | +1.8   |\n",
    "| **boring**   | -1.9   |\n",
    "| **bad**      | -2.0   |\n",
    "| **movie**    | +0.1   |\n",
    "| **not**      | -0.3   |\n",
    "\n",
    "Теперь поступает новый отзыв:\n",
    "\n",
    "> **Отзыв**: *\"This movie is not amazing.\"*\n",
    "\n",
    "### Как модель считает?\n",
    "\n",
    "1. **Выделяет слова**: `[\"movie\", \"not\", \"amazing\"]`\n",
    "2. **Складывает веса**:  \n",
    "   `z = вес(\"movie\") + вес(\"not\") + вес(\"amazing\")`  \n",
    "   `z = 0.1 + (-0.3) + 2.1 = 1.9`\n",
    "3. **Пропускает через сигмоиду**:  \n",
    "   `P(позитив) = 1 / (1 + exp(-1.9)) ≈ 0.87`\n",
    "\n",
    "Вероятность позитивного отзыва — **87%**, поэтому модель предсказывает: **позитивный**.\n",
    "\n",
    "> Но подожди! Отзыв на самом деле **ироничный** (\"not amazing\" = плохо).  \n",
    "> Логистическая регрессия **не улавливает контекст и порядок слов**, если не использовать n-граммы!  \n",
    "> Именно поэтому в нашем векторизаторе мы включили **би- и триграммы** — чтобы фраза *\"not amazing\"* получила **свой собственный вес** (скорее всего, отрицательный).\n",
    "\n",
    "### Вывод\n",
    "\n",
    "Модель работает как **взвешенный подсчёт слов**, где каждое слово «голосует» за позитив или негатив.  \n",
    "С n-граммами она начинает «понимать» и устойчивые фразы — и становится гораздо точнее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413f54ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/imdb_reviews.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Загрузка данных\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/imdb_reviews.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocessor\u001b[39m(text):\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mfloat\u001b[39m):  \u001b[38;5;66;03m# Защита от NaN\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Denis\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Denis\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Denis\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Denis\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Denis\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/imdb_reviews.csv'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "# Загрузка данных\n",
    "\n",
    "df = pd.read_csv('data/imdb_reviews.csv', encoding='utf-8')\n",
    "\n",
    "\n",
    "def preprocessor(text):\n",
    "    if isinstance(text, float):  # Защита от NaN\n",
    "        return \"\"\n",
    "    # Удаляем HTML-теги\n",
    "    text = re.sub(r'<[^>]*>', '', text)\n",
    "    \n",
    "    # Ищем смайлики (эмотиконы) \n",
    "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    \n",
    "    # Удаляем все не-буквенные символы \n",
    "    text = re.sub(r'[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', '')\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# Очищаем все отзывы\n",
    "print(\"Очищаем текст...\")\n",
    "df['cleaned_review'] = df['review'].apply(preprocessor)\n",
    "\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['cleaned_review'], df['sentiment'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Векторизация текста\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,      # берем 5000 самых частых слов\n",
    "    ngram_range=(1, 3),     # учитываем 1,2,3-граммы\n",
    "    stop_words='english'    # удаляем слова-паразиты\n",
    ")\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "# Пример для Logistic Regression\n",
    "param_grid = {'C': [0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=3, scoring='roc_auc')\n",
    "grid.fit(X_train_vec, y_train)\n",
    "print(\"Лучший C:\", grid.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
